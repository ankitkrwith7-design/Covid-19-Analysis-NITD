# ğŸ¦  COVID-19 Data Analysis â€” Big Data Analytics Project

### **Course Code: CSBB 422 â€“ Big Data Analytics**

A Big Data Analytics project focusing on processing, analyzing, and visualizing **COVID-19 datasets** using **Hadoop, Spark, and related tools**.
This project demonstrates the ability to handle large datasets, run distributed analytics, and build meaningful insights on pandemic trends.

---

## ğŸš€ **Project Overview**

This project processes large-scale COVID-19 datasets to analyze:

* Daily case trends
* Recovery rate patterns
* Mortality trends
* Region-wise comparison
* Growth curves & correlations
* Visualizations for meaningful insights

The system leverages **Big Data technologies** to perform distributed processing on multi-node Hadoop/Spark clusters.

---

## ğŸ§° **Tech Stack & Tools Used**

### **Big Data Components**

* **Hadoop HDFS** â€“ Distributed file storage
* **Hadoop MapReduce** â€“ Distributed batch processing
* **Apache Spark (PySpark)** â€“ Fast in-memory analytics
* **Kafka (optional)** â€“ For streaming/real-time ingestion
* **HBase (optional)** â€“ NoSQL storage

### **Development Tools**

* Python
* Jupyter Notebook / PySpark shell
* Matplotlib / Seaborn / Pandas (for visualization)
* Linux environment / Ubuntu
* GitHub for version control & submission

---

## ğŸ—‚ï¸ **Project Features**

âœ”ï¸ Load and store large COVID-19 datasets in **HDFS**
âœ”ï¸ Clean and preprocess raw CSV data
âœ”ï¸ Perform distributed analysis using **Spark RDD/DataFrame API**
âœ”ï¸ Calculate:

* Daily new cases
* 7-day rolling averages
* State/Region-wise comparison
* Fatality & recovery analysis
  âœ”ï¸ Generate graphical insights
  âœ”ï¸ Optional: Real-time ingestion using **Kafka â†’ Spark Streaming**

---

## ğŸ–¥ï¸ **Cluster Setup (As required in project)**

* **1 Master Node**
* **2 Worker Nodes**
* Hadoop fully configured (core-site, hdfs-site, yarn-site)
* Spark installed on all nodes
* SSH passwordless communication enabled

---

## ğŸ“‚ **Repository Structure**

```
Covid-19-Analysis/
â”‚
â”œâ”€â”€ data/                   # Raw COVID datasets (optional placeholder)
â”œâ”€â”€ hdfs/                   # Dataset upload instructions for HDFS
â”œâ”€â”€ spark_scripts/          # PySpark scripts for analysis
â”œâ”€â”€ mapreduce/              # Optional MR code if used
â”œâ”€â”€ visualizations/         # Plots and images
â”œâ”€â”€ ui/                     # UI or dashboard (optional)
â”œâ”€â”€ output/                 # Result files
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ“Š **Analytical Tasks Performed**

### 1ï¸âƒ£ **Data Preprocessing**

* Null value handling
* Date formatting
* Region normalization

### 2ï¸âƒ£ **Spark-Based Analysis**

* Total cases, recoveries, deaths
* Daily & cumulative statistics
* Region-wise ranking

### 3ï¸âƒ£ **Visualization**

* Line graphs for trends
* Bar charts for comparisons
* Heatmaps for intensity

---

## ğŸ› ï¸ **How to Run the Project**

### **Step 1: Upload dataset to HDFS**

```bash
hdfs dfs -mkdir /covid
hdfs dfs -put covid_data.csv /covid/
```

### **Step 2: Run Spark Job**

```bash
spark-submit spark_scripts/covid_analysis.py
```

### **Step 3: View Output**

```bash
hdfs dfs -cat /covid/output/*
```



## ğŸ¯ **Learning Outcomes**

By completing this project, the following competencies were achieved:

* Setup & configuration of **Hadoop, Spark, Kafka, HBase**
* Distributed storage & computing
* Real-time vs batch processing
* Big Data pipeline building
* Data analysis and interpretation
* Using GitHub for submission & version management

---

## ğŸ§‘â€ğŸ’» **Project Team**

* **Adarsh 221210007**
* **Ankit 221210020**
* **Anupam 221210023**
* * **Ankit 221210058**
 
---

## ğŸ“œ **License**

This project is for academic learning under the Big Data Analytics lab course **CSBB 422**.



